Question 1
Interacting with Large Language Models (LLMs) differs from traditional machine learning models.  Working with LLMs involves natural language input, known as a  _____, resulting in output from the Large Language Model, known as the ______ .

ANS-
prompt, completion


Question 2
Large Language Models (LLMs) are capable of performing multiple tasks supporting a variety of use cases.  Which of the following tasks supports the use case of converting code comments into executable code?

ANS-

Question 3
What is the self-attention that powers the transformer architecture?

A mechanism that allows a model to focus on different parts of the input sequence during computation.


Question 4
Which of the following stages are part of the generative AI model lifecycle mentioned in the course? (Select all that apply)


Defining the problem and identifying relevant datasets.

Manipulating the model to align with specific project needs.

Selecting a candidate model and potentially pre-training a custom model.

Deploying the model into the infrastructure and integrating it with the application.

5. "RNNs are better than Transformers for generative AI Tasks." 
FALSE



Question 6
Which transformer-based model architecture has the objective of guessing a masked token based on the previous sequence of tokens by building bidirectional representations of the input sequence.


Autoencoder


QUES 7
Which transformer-based model architecture is well-suited to the task of text translation?

Sequence-to-sequence


Question 8
Do we always need to increase the model size to improve its performance?
FALSE

Question 9
Scaling laws for pre-training large language models consider several aspects to maximize performance of a model within a set of constraints and available scaling choices.  Select all alternatives that should be considered for scaling when performing model pre-training?


Model size: Number of parameters

Compute budget: Compute constraints


Dataset size: Number of tokens

QUESTION 10
"You can combine data parallelism with model parallelism to train LLMs."

Is this true or false?

TRUE
